{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "import argparse\n",
    "from transformers import (\n",
    "    Swinv2ForImageClassification,\n",
    "    AutoImageProcessor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path):\n",
    "    # Load model similarly to ModelEvaluator; using from_pretrained\n",
    "    model = Swinv2ForImageClassification.from_pretrained(checkpoint_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def export_serialized_model(model, output_file):\n",
    "    # Save the model's state dictionary to output_file\n",
    "    # torch.save(model.state_dict(), output_file)\n",
    "    state_dict = model.state_dict()\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if not k.startswith(\"model.\"):\n",
    "            new_state_dict[\"model.\" + k] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "    torch.save(new_state_dict, output_file)\n",
    "\n",
    "\n",
    "def export_ensemble_models(model_states, output_file):\n",
    "    # Save ensemble (dict of state_dicts) to output_file\n",
    "    torch.save(model_states, output_file)\n",
    "\n",
    "\n",
    "def export_to_onnx(model, output_file, checkpoint_path):\n",
    "    # Derive dummy input tensor shape from the image processor\n",
    "    image_processor = AutoImageProcessor.from_pretrained(checkpoint_path)\n",
    "    if \"shortest_edge\" in image_processor.size:\n",
    "        size = image_processor.size[\"shortest_edge\"]\n",
    "    elif \"height\" in image_processor.size and \"width\" in image_processor.size:\n",
    "        s = image_processor.size\n",
    "        size = (s[\"height\"], s[\"width\"])\n",
    "    else:\n",
    "        size = 224  # fallback to default size\n",
    "    if isinstance(size, int):\n",
    "        dummy_input = torch.randn(1, 3, size, size)\n",
    "    else:\n",
    "        dummy_input = torch.randn(1, 3, size[0], size[1])\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        output_file,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        opset_version=11,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_model_archiver(\n",
    "    model_name,\n",
    "    version,\n",
    "    serialized_file,\n",
    "    export_path,\n",
    "    handler,\n",
    "    requirements=\"\",\n",
    "    config=\"\",\n",
    "    extra_files=\"\",\n",
    "):\n",
    "    # Build and run the torch-model-archiver command\n",
    "    cmd = [\n",
    "        \"torch-model-archiver\",\n",
    "        \"--model-name\",\n",
    "        model_name,\n",
    "        \"--version\",\n",
    "        version,\n",
    "        \"--serialized-file\",\n",
    "        serialized_file,\n",
    "        \"--handler\",\n",
    "        handler,\n",
    "        \"--export-path\",\n",
    "        export_path,\n",
    "        \"-f\",\n",
    "    ]\n",
    "    if extra_files:\n",
    "        cmd.extend([\"--extra-files\", extra_files])\n",
    "    if requirements:\n",
    "        cmd.extend([\"--requirements-file\", requirements])\n",
    "    if config:\n",
    "        cmd.extend([\"--config\", config])\n",
    "    subprocess.run(cmd, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"Export and archive a PyTorch model.\")\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint_path\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the model checkpoint or directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name\", type=str, required=True, help=\"Name for the model archive.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--version\", type=str, default=\"1.0\", help=\"Version for the model archive.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--extra_files\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Extra files to include (comma separated if multiple).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--requirements\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Requirements file for the model archive.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ensemble\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Enable ensemble mode to save multiple models in one serialized file.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--export_onnx\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Export the model to ONNX format (disabled in ensemble mode).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--onnx_output\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Output file for ONNX model export.\",\n",
    "    )\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Exporting model...\")\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # print(\"Test args\")\n",
    "    # base_model_path = \"../environments/torchserve/gpu/artifacts\"\n",
    "    # model_name = \"27spp_model_1\"\n",
    "    # files = [\n",
    "    #     f\"{base_model_path}/config.properties\",\n",
    "    #     f\"{base_model_path}/index_to_name.json\",\n",
    "    #     f\"{base_model_path}/config.json\",\n",
    "    #     f\"{base_model_path}/model.safetensors\",\n",
    "    #     f\"{base_model_path}/preprocessor_config.json\",\n",
    "    #     f\"{base_model_path}/{model_name}_serialized.pt\",\n",
    "    # ]\n",
    "    # filepaths = \",\".join(files)\n",
    "    # argarr = [\n",
    "    #     f\"--checkpoint_path {base_model_path}\",\n",
    "    #     f\"--serialized_output {base_model_path}/{model_name}_serialized.pt\",\n",
    "    #     f\"--model_name {model_name}\",\n",
    "    #     # \"--model_file ../environments/torchserve/gpu/artifacts/model.py\",\n",
    "    #     \"--version 1.0\",\n",
    "    #     f\"--handler {base_model_path}/model_handler.py\",\n",
    "    #     f\"--export_path {base_model_path}/\",\n",
    "    #     \"--extra_files \" + filepaths,\n",
    "    # ]\n",
    "    # argstr = \" \".join(argarr)\n",
    "\n",
    "    # print(\"Parsing args\")\n",
    "    # args = parser.parse_args(argstr.split())\n",
    "\n",
    "    if args.ensemble:\n",
    "        ensemble_states = {}\n",
    "        # Assume checkpoints are subdirectories in checkpoint_path\n",
    "        for subdir in os.listdir(args.checkpoint_path):\n",
    "            sub_path = os.path.join(args.checkpoint_path, subdir)\n",
    "            if os.path.isdir(sub_path):\n",
    "                print(f\"Loading model from {sub_path}...\")\n",
    "                model = load_model(sub_path)\n",
    "                ensemble_states[subdir] = model.state_dict()\n",
    "        print(\"Serializing ensemble models...\")\n",
    "        export_ensemble_models(ensemble_states, args.serialized_output)\n",
    "    else:\n",
    "        print(\"Loading model...\")\n",
    "        model = load_model(args.checkpoint_path)\n",
    "        print(\"Serializing model...\")\n",
    "        export_serialized_model(model, args.serialized_output)\n",
    "        if args.export_onnx:\n",
    "            print(\"Exporting model to ONNX format...\")\n",
    "            export_to_onnx(model, args.onnx_output, args.checkpoint_path)\n",
    "\n",
    "    os.makedirs(args.export_path, exist_ok=True)\n",
    "\n",
    "    files = [\n",
    "        f\"{args.checkpoint_path}/config.properties\",\n",
    "        f\"{args.checkpoint_path}/index_to_name.json\",\n",
    "        f\"{args.checkpoint_path}/config.json\",\n",
    "        f\"{args.checkpoint_path}/model.safetensors\",\n",
    "        f\"{args.checkpoint_path}/preprocessor_config.json\",\n",
    "        # f\"{args.checkpoint_path}/{args.model_name}_serialized.pt\",\n",
    "    ]\n",
    "    extra_files = \",\".join(files)\n",
    "    if args.extra_files:\n",
    "        extra_files += \",\" + args\n",
    "\n",
    "    print(\"Archiving model via torch-model-archiver...\")\n",
    "    run_model_archiver(\n",
    "        model_name=args.model_name,\n",
    "        version=args.version,\n",
    "        requirements=args.requirements,\n",
    "        export_path=f\"{args.checkpoint_path}/\",\n",
    "        handler=f\"{args.checkpoint_path}/model_handler.py\",\n",
    "        config=f\"{args.checkpoint_path}/config.properties\",\n",
    "        serialized_file=f\"{args.checkpoint_path}/{args.model_name}_serialized.pt\",\n",
    "        extra_files=extra_files,\n",
    "    )\n",
    "\n",
    "    print(\"Model archive created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
