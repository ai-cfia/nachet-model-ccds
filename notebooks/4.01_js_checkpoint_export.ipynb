{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import subprocess\n",
    "import argparse\n",
    "from transformers import (\n",
    "    Swinv2ForImageClassification,\n",
    "    AutoImageProcessor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path):\n",
    "    # Load model similarly to ModelEvaluator; using from_pretrained\n",
    "    model = Swinv2ForImageClassification.from_pretrained(checkpoint_path)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def export_serialized_model(model, output_file):\n",
    "    # Save the model's state dictionary to output_file\n",
    "    torch.save(model.state_dict(), output_file)\n",
    "\n",
    "\n",
    "def export_ensemble_models(model_states, output_file):\n",
    "    # Save ensemble (dict of state_dicts) to output_file\n",
    "    torch.save(model_states, output_file)\n",
    "\n",
    "\n",
    "def export_to_onnx(model, output_file, checkpoint_path):\n",
    "    # Derive dummy input tensor shape from the image processor\n",
    "    image_processor = AutoImageProcessor.from_pretrained(checkpoint_path)\n",
    "    if \"shortest_edge\" in image_processor.size:\n",
    "        size = image_processor.size[\"shortest_edge\"]\n",
    "    elif \"height\" in image_processor.size and \"width\" in image_processor.size:\n",
    "        s = image_processor.size\n",
    "        size = (s[\"height\"], s[\"width\"])\n",
    "    else:\n",
    "        size = 224  # fallback to default size\n",
    "    if isinstance(size, int):\n",
    "        dummy_input = torch.randn(1, 3, size, size)\n",
    "    else:\n",
    "        dummy_input = torch.randn(1, 3, size[0], size[1])\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        output_file,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        opset_version=11,\n",
    "    )\n",
    "\n",
    "\n",
    "def run_model_archiver(\n",
    "    model_name,\n",
    "    version,\n",
    "    model_file,\n",
    "    serialized_file,\n",
    "    export_path,\n",
    "    handler,\n",
    "    requirements=\"\",\n",
    "    config=\"\",\n",
    "    extra_files=\"\",\n",
    "):\n",
    "    # Build and run the torch-model-archiver command\n",
    "    cmd = [\n",
    "        \"torch-model-archiver\",\n",
    "        \"--model-name\",\n",
    "        model_name,\n",
    "        \"--version\",\n",
    "        version,\n",
    "        \"--model-file\",\n",
    "        model_file,\n",
    "        \"--serialized-file\",\n",
    "        serialized_file,\n",
    "        \"--handler\",\n",
    "        handler,\n",
    "        \"--export-path\",\n",
    "        export_path,\n",
    "        \"-f\",\n",
    "    ]\n",
    "    if extra_files:\n",
    "        cmd.extend([\"--extra-files\", extra_files])\n",
    "    if requirements:\n",
    "        cmd.extend([\"--requirements-file\", requirements])\n",
    "    if config:\n",
    "        cmd.extend([\"--config\", config])\n",
    "    subprocess.run(cmd, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"Export and archive a PyTorch model.\")\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint_path\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the model checkpoint or directory\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--serialized_output\",\n",
    "        type=str,\n",
    "        default=\"model_serialized.pt\",\n",
    "        help=\"Output file for serialized model.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name\", type=str, required=True, help=\"Name for the model archive.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--version\", type=str, default=\"1.0\", help=\"Version for the model archive.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--handler\",\n",
    "        type=str,\n",
    "        default=\"handler.py\",\n",
    "        help=\"Handler file required for model archiver.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--export_path\",\n",
    "        type=str,\n",
    "        default=\"model_store\",\n",
    "        help=\"Directory to store the model archive.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--extra_files\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Extra files to include (comma separated if multiple).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--requirements\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Requirements file for the model archive.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        type=str,\n",
    "        default=\"\",\n",
    "        help=\"Config file for the model archive.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ensemble\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Enable ensemble mode to save multiple models in one serialized file.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--export_onnx\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Export the model to ONNX format (disabled in ensemble mode).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--onnx_output\",\n",
    "        type=str,\n",
    "        default=\"model.onnx\",\n",
    "        help=\"Output file for ONNX model export.\",\n",
    "    )\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = get_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.ensemble:\n",
    "        ensemble_states = {}\n",
    "        # Assume checkpoints are subdirectories in checkpoint_path\n",
    "        for subdir in os.listdir(args.checkpoint_path):\n",
    "            sub_path = os.path.join(args.checkpoint_path, subdir)\n",
    "            if os.path.isdir(sub_path):\n",
    "                print(f\"Loading model from {sub_path}...\")\n",
    "                model = load_model(sub_path)\n",
    "                ensemble_states[subdir] = model.state_dict()\n",
    "        print(\"Serializing ensemble models...\")\n",
    "        export_ensemble_models(ensemble_states, args.serialized_output)\n",
    "    else:\n",
    "        print(\"Loading model...\")\n",
    "        model = load_model(args.checkpoint_path)\n",
    "        print(\"Serializing model...\")\n",
    "        export_serialized_model(model, args.serialized_output)\n",
    "        if args.export_onnx:\n",
    "            print(\"Exporting model to ONNX format...\")\n",
    "            export_to_onnx(model, args.onnx_output, args.checkpoint_path)\n",
    "\n",
    "    os.makedirs(args.export_path, exist_ok=True)\n",
    "\n",
    "    print(\"Archiving model via torch-model-archiver...\")\n",
    "    run_model_archiver(\n",
    "        args.model_name,\n",
    "        args.version,\n",
    "        args.serialized_output,\n",
    "        args.export_path,\n",
    "        args.handler,\n",
    "        args.requirements,\n",
    "        args.config,\n",
    "        args.extra_files,\n",
    "    )\n",
    "\n",
    "    print(\"Model archive created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
